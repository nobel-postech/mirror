{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import iglob\n",
    "from ast import literal_eval\n",
    "\n",
    "from os.path import join as pjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/kimsubin/mm_counselor/mirror\"\n",
    "ANNOT_DIR = pjoin(ROOT_DIR, \"annot_data\")\n",
    "\n",
    "POSTPROC_RESULT_DIR = pjoin(ANNOT_DIR, \"utterfeat/postprocess\", \"results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_into_chunks(lst, chunk_size=2):\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def write_jsonl(save_path, json_obj):\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        for entry in json_obj:\n",
    "            json.dump(entry, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    return\n",
    "\n",
    "def write_line(path, entry):\n",
    "    with open(path, 'a+') as f:\n",
    "        json.dump(entry, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirror = pd.read_csv(pjoin(ANNOT_DIR, \"mirror.csv\"), converters={\n",
    "    'proc_dialogue': literal_eval\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Basic Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def filter_turn_count(conversation: str) -> bool:\n",
    "    \"\"\"\n",
    "    Remove conversations with less than 4 or more than 20 turns.\n",
    "    \"\"\"\n",
    "    turns = conversation.splitlines()\n",
    "    turns = list(filter(lambda x: len(x.strip()) > 0, turns))\n",
    "    return 4 <= len(turns) / 2 <= 20\n",
    "\n",
    "def filter_speaker_count(conversation: str) -> bool:\n",
    "    \"\"\"\n",
    "    Remove conversations with more than two distinct speakers.\n",
    "    \"\"\"\n",
    "    speakers = set()\n",
    "    turns = conversation.splitlines()\n",
    "    turns = list(filter(lambda x: len(x.strip()) > 0, turns))\n",
    "    for line in turns:\n",
    "        match = re.match(r\"^(\\\\w+):\", line)\n",
    "        if match:\n",
    "            speakers.add(match.group(1))\n",
    "    return len(speakers) <= 2\n",
    "\n",
    "\n",
    "def filter_repetition(conversation: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect excessive repetition within individual utterances.\n",
    "    \"\"\"\n",
    "    turns = conversation.splitlines()\n",
    "    turns = list(filter(lambda x: len(x.strip()) > 0, turns))\n",
    "    for line in turns:\n",
    "        speaker, _, utterance = line.partition(\":\")\n",
    "        if utterance:\n",
    "            # Normalize text and split by spaces for simple word repetition detection\n",
    "            words = utterance.lower().split()\n",
    "            unique_words = set(words)\n",
    "            if len(unique_words) / len(words) < 0.5:  # More than 50% repetition\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def filter_response_failure(conversation: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect response failures where the context is mismatched or missing.\n",
    "    Simple lexical analysis can identify potential issues.\n",
    "    \"\"\"\n",
    "    turns = conversation.splitlines()\n",
    "    turns = list(filter(lambda x: len(x.strip()) > 0, turns))\n",
    "    if len(turns) < 2:\n",
    "        return True  # Not enough context to analyze\n",
    "    \n",
    "    for i in range(1, len(turns)):\n",
    "        _, _, prev_utterance = turns[i - 1].partition(\":\")\n",
    "        speaker, _, curr_utterance = turns[i].partition(\":\")\n",
    "        if prev_utterance.strip() and curr_utterance.strip():\n",
    "            # Simple heuristic: check if response is \"I don't know\", \"uhh\", or empty-like\n",
    "            if speaker.lower().strip().startswith('therapist') \\\n",
    "                and re.match(r\"^(i don\\\\'t know|uh+|hmm+|)$\", curr_utterance.strip(), re.IGNORECASE):\n",
    "                return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Basic Filtering:   0%|          | 0/41223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Basic Filtering: 100%|██████████| 41223/41223 [00:05<00:00, 7389.96it/s]\n"
     ]
    }
   ],
   "source": [
    "drop = []\n",
    "for i, row in tqdm(mirror.iterrows(), total=len(mirror), desc=\"Basic Filtering\"):\n",
    "    conversation = row['dialogue'].strip()\n",
    "    turn_result = filter_turn_count(conversation)\n",
    "    repetition_result = filter_repetition(conversation)\n",
    "    erroneous_result = filter_response_failure(conversation)\n",
    "    speaker_result = filter_speaker_count(conversation)\n",
    "    if not turn_result:\n",
    "        drop += [{'idx': row['idx'], 'reason' : 'Turn Count'}]\n",
    "    if not repetition_result:\n",
    "        drop += [{'idx': row['idx'], 'reason' : 'Repetition Result'}]\n",
    "    if not erroneous_result:\n",
    "        drop += [{'idx': row['idx'], 'reason' : 'Response Failure'}]\n",
    "    if not speaker_result:\n",
    "        drop += [{'idx': row['idx'], 'reason' : 'Speaker Count'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(map(lambda x: x['idx'], drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Consequenc POS Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kimsubin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kimsubin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/kimsubin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos_repetition(conversation: str, max_repeats=3) -> bool:\n",
    "    turns = conversation.splitlines()\n",
    "    turns = list(filter(lambda x: len(x.strip()) > 0, turns))\n",
    "    for line in turns:\n",
    "        speaker, _, utterance = line.partition(\":\")\n",
    "        if not utterance.strip():\n",
    "            continue  # Skip empty lines or lines without an utterance\n",
    "\n",
    "        # Tokenize and POS-tag the utterance\n",
    "        tokens = word_tokenize(utterance.strip())\n",
    "        pos_tags = pos_tag(tokens)\n",
    "\n",
    "        # Check for consecutive repetitions of specified POS tags\n",
    "        consecutive_count = 0\n",
    "        previous_pos = None\n",
    "        for token, pos in pos_tags:\n",
    "            if token in ['“', '”', '’', '‘', ']', '[']: \n",
    "                consecutive_count = 1\n",
    "                previous_pos = None\n",
    "                continue\n",
    "            if pos == previous_pos:\n",
    "                consecutive_count += 1\n",
    "                if consecutive_count > max_repeats:\n",
    "                    return False  # Excessive POS repetition detected\n",
    "            else:\n",
    "                consecutive_count = 1  # Reset count for new POS\n",
    "            previous_pos = pos\n",
    "    return True  # No excessive POS repetition detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = []\n",
    "for i, row in tqdm(mirror.iterrows(), total=len(mirror), desc=\"Basic Filtering\"):\n",
    "    conversation = row['dialogue'].strip()\n",
    "    pos_repetition_result = filter_pos_repetition(conversation=conversation, max_repeats=3)\n",
    "    if not pos_repetition_result:\n",
    "        drop += [row['idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(json_obj=drop, save_path=pjoin(POSTPROC_RESULT_DIR, \"pos\", \"drop.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_results = [json.loads(q) for q in open(pjoin(POSTPROC_RESULT_DIR, \"pos\", 'drop.jsonl'), 'r')]\n",
    "len(drop_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in drop_results:\n",
    "    write_line(path=pjoin(POSTPROC_RESULT_DIR, \"drop_idx.jsonl\"), entry={'type': 'repetitive pos',\n",
    "                                                                     'idx': idx})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Safety Filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "canary_result = [json.loads(q) for q in open(pjoin(POSTPROC_RESULT_DIR, \"canary\", \"results.jsonl\"), 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_safety(safety_str):\n",
    "    safety, reason = safety_str, None\n",
    "    if ' ' in safety:\n",
    "        safety, reason = safety.split(\" \", 1)\n",
    "\n",
    "    return safety, reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_labels = {\"__casual__\", \"__possibly_needs_caution__\", \"__probably_needs_caution__\", \"__needs_caution__\", \"__needs_intervention__\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Postprocessing Canary Results:   0%|          | 0/41223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Postprocessing Canary Results: 100%|██████████| 41223/41223 [00:00<00:00, 294783.62it/s]\n"
     ]
    }
   ],
   "source": [
    "therapist_safety_dic = {}\n",
    "drop_candidates_t = []\n",
    "\n",
    "for row in tqdm(canary_result, total=len(canary_result), desc=\"Postprocessing Canary Results\"):\n",
    "    idx = row['idx']\n",
    "\n",
    "    for utt in row['therapist_safety']:\n",
    "        safety, reason = parse_safety(utt['safety'])\n",
    "        if not safety in [\"__needs_caution__\", \"__needs_intervention__\"]: continue\n",
    "        drop_candidates_t += [idx]\n",
    "\n",
    "        if safety in therapist_safety_dic:\n",
    "            therapist_safety_dic[safety] += [utt]\n",
    "        else:\n",
    "            therapist_safety_dic[safety] = [utt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452, 41223)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(drop_candidates_t)), len(canary_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in set(drop_candidates_t):\n",
    "    write_line(path=pjoin(POSTPROC_RESULT_DIR, \"drop_idx.jsonl\"), \n",
    "               entry={'type': 'need intervention', 'idx': idx})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Copy-Paste Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_content = ['personal_info', 'personality', 'reason_for_seeking_counseling', 'distorted_thought', 'cbt_plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41223/41223 [00:52<00:00, 781.64it/s] \n"
     ]
    }
   ],
   "source": [
    "drop = []\n",
    "for i, row in tqdm(mirror.iterrows(), total=len(mirror)):\n",
    "    for t, utt in enumerate(row['proc_dialogue']):\n",
    "        for k in prompt_content:\n",
    "            check_sents = split_into_sentences(row[k])\n",
    "            for s in check_sents:\n",
    "                if s in utt['statement']: \n",
    "                    if s.startswith(tuple([f\"{n}.\"for n in range(11)])): continue\n",
    "                    drop += [row['idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(json_obj=list(set(drop)), save_path=pjoin(POSTPROC_RESULT_DIR, \"copy-paste\", \"drop.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in set(drop):\n",
    "    write_line(path=pjoin(POSTPROC_RESULT_DIR, \"drop_idx.jsonl\"), \n",
    "               entry={'type': 'copy-paste', 'idx': idx})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
